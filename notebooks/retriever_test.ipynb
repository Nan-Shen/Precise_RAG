{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aim:\n",
    "Assess the performance of different RAG methods in financial documents analysis (primary background investigation for consulting, stock purchase) \n",
    "\n",
    "# Assessment criteria:\n",
    "1. relevance\n",
    "2. length of retrieved context\n",
    "3. speed\n",
    "4. cost\n",
    "\n",
    "# Methods to test:\n",
    "1. Dense Embeddings\n",
    "   1.1 parameters\n",
    "   1.2 finetune embedding model (need GPU machine, too expensive for now)\n",
    "2. ColBERT\n",
    "4. Hybrid retriever and rerank\n",
    "5. Knowledge Augmented Generation (KAG, need to build a domain-specific architecture from sratch)\n",
    "6. Contextual retrieval preprocessing (use llm to search through all chunks, too expensive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Rank (Borda)</th>\n",
       "      <th>Model</th>\n",
       "      <th>Zero-shot</th>\n",
       "      <th>Number of Parameters</th>\n",
       "      <th>Embedding Dimensions</th>\n",
       "      <th>Max Tokens</th>\n",
       "      <th>Mean (Task)</th>\n",
       "      <th>Mean (TaskType)</th>\n",
       "      <th>Bitext Mining</th>\n",
       "      <th>Classification</th>\n",
       "      <th>Clustering</th>\n",
       "      <th>Instruction Retrieval</th>\n",
       "      <th>Multilabel Classification</th>\n",
       "      <th>Pair Classification</th>\n",
       "      <th>Reranking</th>\n",
       "      <th>Retrieval</th>\n",
       "      <th>STS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[Linq-Embed-Mistral](https://huggingface.co/Li...</td>\n",
       "      <td>99</td>\n",
       "      <td>7B</td>\n",
       "      <td>4096</td>\n",
       "      <td>32768</td>\n",
       "      <td>61.47</td>\n",
       "      <td>54.21</td>\n",
       "      <td>70.34</td>\n",
       "      <td>62.24</td>\n",
       "      <td>51.27</td>\n",
       "      <td>0.94</td>\n",
       "      <td>24.77</td>\n",
       "      <td>80.43</td>\n",
       "      <td>64.37</td>\n",
       "      <td>58.69</td>\n",
       "      <td>74.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[gte-Qwen2-7B-instruct](https://huggingface.co...</td>\n",
       "      <td>-1</td>\n",
       "      <td>7B</td>\n",
       "      <td>3584</td>\n",
       "      <td>32768</td>\n",
       "      <td>62.51</td>\n",
       "      <td>56.00</td>\n",
       "      <td>73.92</td>\n",
       "      <td>61.55</td>\n",
       "      <td>53.36</td>\n",
       "      <td>4.94</td>\n",
       "      <td>25.48</td>\n",
       "      <td>85.13</td>\n",
       "      <td>65.55</td>\n",
       "      <td>60.08</td>\n",
       "      <td>73.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>[multilingual-e5-large-instruct](https://huggi...</td>\n",
       "      <td>99</td>\n",
       "      <td>560M</td>\n",
       "      <td>1024</td>\n",
       "      <td>514</td>\n",
       "      <td>63.23</td>\n",
       "      <td>55.17</td>\n",
       "      <td>80.13</td>\n",
       "      <td>64.94</td>\n",
       "      <td>51.54</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>22.91</td>\n",
       "      <td>80.86</td>\n",
       "      <td>62.61</td>\n",
       "      <td>57.12</td>\n",
       "      <td>76.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>[SFR-Embedding-Mistral](https://huggingface.co...</td>\n",
       "      <td>96</td>\n",
       "      <td>7B</td>\n",
       "      <td>4096</td>\n",
       "      <td>32768</td>\n",
       "      <td>60.93</td>\n",
       "      <td>54.00</td>\n",
       "      <td>70.00</td>\n",
       "      <td>60.02</td>\n",
       "      <td>52.57</td>\n",
       "      <td>0.16</td>\n",
       "      <td>24.55</td>\n",
       "      <td>80.29</td>\n",
       "      <td>64.19</td>\n",
       "      <td>59.44</td>\n",
       "      <td>74.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>[GritLM-7B](https://huggingface.co/GritLM/Grit...</td>\n",
       "      <td>99</td>\n",
       "      <td>7B</td>\n",
       "      <td>4096</td>\n",
       "      <td>4096</td>\n",
       "      <td>60.93</td>\n",
       "      <td>53.83</td>\n",
       "      <td>70.53</td>\n",
       "      <td>61.83</td>\n",
       "      <td>50.48</td>\n",
       "      <td>3.45</td>\n",
       "      <td>22.77</td>\n",
       "      <td>79.94</td>\n",
       "      <td>63.78</td>\n",
       "      <td>58.31</td>\n",
       "      <td>73.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Rank (Borda)  \\\n",
       "0           0             1   \n",
       "1           1             2   \n",
       "2           2             3   \n",
       "3           3             4   \n",
       "4           4             5   \n",
       "\n",
       "                                               Model  Zero-shot  \\\n",
       "0  [Linq-Embed-Mistral](https://huggingface.co/Li...         99   \n",
       "1  [gte-Qwen2-7B-instruct](https://huggingface.co...         -1   \n",
       "2  [multilingual-e5-large-instruct](https://huggi...         99   \n",
       "3  [SFR-Embedding-Mistral](https://huggingface.co...         96   \n",
       "4  [GritLM-7B](https://huggingface.co/GritLM/Grit...         99   \n",
       "\n",
       "  Number of Parameters Embedding Dimensions Max Tokens  Mean (Task)  \\\n",
       "0                   7B                 4096      32768        61.47   \n",
       "1                   7B                 3584      32768        62.51   \n",
       "2                 560M                 1024        514        63.23   \n",
       "3                   7B                 4096      32768        60.93   \n",
       "4                   7B                 4096       4096        60.93   \n",
       "\n",
       "   Mean (TaskType)  Bitext Mining  Classification  Clustering  \\\n",
       "0            54.21          70.34           62.24       51.27   \n",
       "1            56.00          73.92           61.55       53.36   \n",
       "2            55.17          80.13           64.94       51.54   \n",
       "3            54.00          70.00           60.02       52.57   \n",
       "4            53.83          70.53           61.83       50.48   \n",
       "\n",
       "   Instruction Retrieval  Multilabel Classification  Pair Classification  \\\n",
       "0                   0.94                      24.77                80.43   \n",
       "1                   4.94                      25.48                85.13   \n",
       "2                  -0.40                      22.91                80.86   \n",
       "3                   0.16                      24.55                80.29   \n",
       "4                   3.45                      22.77                79.94   \n",
       "\n",
       "   Reranking  Retrieval    STS  \n",
       "0      64.37      58.69  74.86  \n",
       "1      65.55      60.08  73.98  \n",
       "2      62.61      57.12  76.81  \n",
       "3      64.19      59.44  74.79  \n",
       "4      63.78      58.31  73.33  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# https://huggingface.co/spaces/mteb/leaderboard\n",
    "embed_dt = pd.read_csv('../data/tmpsrfsg8rr.csv')\n",
    "embed_dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Rank (Borda)</th>\n",
       "      <th>Model</th>\n",
       "      <th>Zero-shot</th>\n",
       "      <th>Number of Parameters</th>\n",
       "      <th>Embedding Dimensions</th>\n",
       "      <th>Max Tokens</th>\n",
       "      <th>Mean (Task)</th>\n",
       "      <th>Mean (TaskType)</th>\n",
       "      <th>Bitext Mining</th>\n",
       "      <th>Classification</th>\n",
       "      <th>Clustering</th>\n",
       "      <th>Instruction Retrieval</th>\n",
       "      <th>Multilabel Classification</th>\n",
       "      <th>Pair Classification</th>\n",
       "      <th>Reranking</th>\n",
       "      <th>Retrieval</th>\n",
       "      <th>STS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>33</td>\n",
       "      <td>[gte-Qwen1.5-7B-instruct](https://huggingface....</td>\n",
       "      <td>-1</td>\n",
       "      <td>7B</td>\n",
       "      <td>4096</td>\n",
       "      <td>32768</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.98</td>\n",
       "      <td>5.36</td>\n",
       "      <td>23.45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[gte-Qwen2-7B-instruct](https://huggingface.co...</td>\n",
       "      <td>-1</td>\n",
       "      <td>7B</td>\n",
       "      <td>3584</td>\n",
       "      <td>32768</td>\n",
       "      <td>62.51</td>\n",
       "      <td>56.00</td>\n",
       "      <td>73.92</td>\n",
       "      <td>61.55</td>\n",
       "      <td>53.36</td>\n",
       "      <td>4.94</td>\n",
       "      <td>25.48</td>\n",
       "      <td>85.13</td>\n",
       "      <td>65.55</td>\n",
       "      <td>60.08</td>\n",
       "      <td>73.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>221</td>\n",
       "      <td>222</td>\n",
       "      <td>flan-t5-large</td>\n",
       "      <td>100</td>\n",
       "      <td>783M</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>1024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>[GritLM-7B](https://huggingface.co/GritLM/Grit...</td>\n",
       "      <td>99</td>\n",
       "      <td>7B</td>\n",
       "      <td>4096</td>\n",
       "      <td>4096</td>\n",
       "      <td>60.93</td>\n",
       "      <td>53.83</td>\n",
       "      <td>70.53</td>\n",
       "      <td>61.83</td>\n",
       "      <td>50.48</td>\n",
       "      <td>3.45</td>\n",
       "      <td>22.77</td>\n",
       "      <td>79.94</td>\n",
       "      <td>63.78</td>\n",
       "      <td>58.31</td>\n",
       "      <td>73.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>[NV-Embed-v1](https://huggingface.co/nvidia/NV...</td>\n",
       "      <td>92</td>\n",
       "      <td>7B</td>\n",
       "      <td>4096</td>\n",
       "      <td>32768</td>\n",
       "      <td>54.86</td>\n",
       "      <td>48.39</td>\n",
       "      <td>48.90</td>\n",
       "      <td>57.04</td>\n",
       "      <td>43.36</td>\n",
       "      <td>3.02</td>\n",
       "      <td>18.95</td>\n",
       "      <td>76.19</td>\n",
       "      <td>64.29</td>\n",
       "      <td>53.98</td>\n",
       "      <td>69.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  Rank (Borda)  \\\n",
       "32           32            33   \n",
       "1             1             2   \n",
       "221         221           222   \n",
       "4             4             5   \n",
       "25           25            26   \n",
       "\n",
       "                                                 Model  Zero-shot  \\\n",
       "32   [gte-Qwen1.5-7B-instruct](https://huggingface....         -1   \n",
       "1    [gte-Qwen2-7B-instruct](https://huggingface.co...         -1   \n",
       "221                                      flan-t5-large        100   \n",
       "4    [GritLM-7B](https://huggingface.co/GritLM/Grit...         99   \n",
       "25   [NV-Embed-v1](https://huggingface.co/nvidia/NV...         92   \n",
       "\n",
       "    Number of Parameters Embedding Dimensions Max Tokens  Mean (Task)  \\\n",
       "32                    7B                 4096      32768          NaN   \n",
       "1                     7B                 3584      32768        62.51   \n",
       "221                 783M              Unknown       1024          NaN   \n",
       "4                     7B                 4096       4096        60.93   \n",
       "25                    7B                 4096      32768        54.86   \n",
       "\n",
       "     Mean (TaskType)  Bitext Mining  Classification  Clustering  \\\n",
       "32               NaN          60.80             NaN       52.98   \n",
       "1              56.00          73.92           61.55       53.36   \n",
       "221              NaN            NaN             NaN         NaN   \n",
       "4              53.83          70.53           61.83       50.48   \n",
       "25             48.39          48.90           57.04       43.36   \n",
       "\n",
       "     Instruction Retrieval  Multilabel Classification  Pair Classification  \\\n",
       "32                    5.36                      23.45                  NaN   \n",
       "1                     4.94                      25.48                85.13   \n",
       "221                   4.72                        NaN                  NaN   \n",
       "4                     3.45                      22.77                79.94   \n",
       "25                    3.02                      18.95                76.19   \n",
       "\n",
       "     Reranking  Retrieval    STS  \n",
       "32         NaN        NaN    NaN  \n",
       "1        65.55      60.08  73.98  \n",
       "221        NaN        NaN    NaN  \n",
       "4        63.78      58.31  73.33  \n",
       "25       64.29      53.98  69.77  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_dt.sort_values(by=['Instruction Retrieval'], ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['7B', '560M', '57B', 'Unknown', '1B', '559M', '494M', '568M',\n",
       "       '572M', '305M', '278M', '567M', '117M', '118M', '359M', '470M',\n",
       "       '435M', '471M', '107M', '335M', '109M', '33M', '137M', '434M',\n",
       "       '108M', '125M', '124M', '22M', '129M', '32M', '19M', '17M', '30M',\n",
       "       '2B', '103M', '427M', '404M', '15M', '110M', '29M', '7M', '35M',\n",
       "       '3M', '102M', '11M', '135M', '2M', '162M', '98M', '9B', '66M',\n",
       "       '3B', '11B', '6B', '8B', '306M', '149M', '281M', '31M', '4B',\n",
       "       '272M', '74M', '823M', '326M', '783M', '353M', '122M', '248M',\n",
       "       '24M'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_dt['Number of Parameters'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "embed_dt['Number of Parameters'] = embed_dt['Number of Parameters'].replace('Unknown', '0')\n",
    "embed_dt['Number of Parameters'] = embed_dt['Number of Parameters'].apply(lambda x: re.search('[\\\\d]+', x.replace('B', '000'))[0] if 'B' in x else re.search('[\\\\d]+', x)[0])\n",
    "embed_dt['Number of Parameters'] = embed_dt['Number of Parameters'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Rank (Borda)</th>\n",
       "      <th>Model</th>\n",
       "      <th>Zero-shot</th>\n",
       "      <th>Number of Parameters</th>\n",
       "      <th>Embedding Dimensions</th>\n",
       "      <th>Max Tokens</th>\n",
       "      <th>Mean (Task)</th>\n",
       "      <th>Mean (TaskType)</th>\n",
       "      <th>Bitext Mining</th>\n",
       "      <th>Classification</th>\n",
       "      <th>Clustering</th>\n",
       "      <th>Instruction Retrieval</th>\n",
       "      <th>Multilabel Classification</th>\n",
       "      <th>Pair Classification</th>\n",
       "      <th>Reranking</th>\n",
       "      <th>Retrieval</th>\n",
       "      <th>STS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>146</td>\n",
       "      <td>147</td>\n",
       "      <td>[ternary-weight-embedding](https://huggingface...</td>\n",
       "      <td>-1</td>\n",
       "      <td>98</td>\n",
       "      <td>1024</td>\n",
       "      <td>512</td>\n",
       "      <td>31.19</td>\n",
       "      <td>26.71</td>\n",
       "      <td>12.65</td>\n",
       "      <td>39.03</td>\n",
       "      <td>24.85</td>\n",
       "      <td>0.92</td>\n",
       "      <td>12.14</td>\n",
       "      <td>64.88</td>\n",
       "      <td>32.09</td>\n",
       "      <td>10.63</td>\n",
       "      <td>43.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>127</td>\n",
       "      <td>128</td>\n",
       "      <td>[potion-base-4M](https://huggingface.co/minish...</td>\n",
       "      <td>99</td>\n",
       "      <td>3</td>\n",
       "      <td>128</td>\n",
       "      <td>Infinite</td>\n",
       "      <td>37.86</td>\n",
       "      <td>32.29</td>\n",
       "      <td>14.59</td>\n",
       "      <td>43.71</td>\n",
       "      <td>32.90</td>\n",
       "      <td>0.61</td>\n",
       "      <td>11.69</td>\n",
       "      <td>70.80</td>\n",
       "      <td>38.54</td>\n",
       "      <td>27.23</td>\n",
       "      <td>50.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>138</td>\n",
       "      <td>139</td>\n",
       "      <td>[potion-base-2M](https://huggingface.co/minish...</td>\n",
       "      <td>99</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>Infinite</td>\n",
       "      <td>36.33</td>\n",
       "      <td>31.06</td>\n",
       "      <td>12.19</td>\n",
       "      <td>42.25</td>\n",
       "      <td>31.79</td>\n",
       "      <td>0.59</td>\n",
       "      <td>11.53</td>\n",
       "      <td>70.43</td>\n",
       "      <td>37.38</td>\n",
       "      <td>23.99</td>\n",
       "      <td>49.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>124</td>\n",
       "      <td>125</td>\n",
       "      <td>[potion-base-8M](https://huggingface.co/minish...</td>\n",
       "      <td>99</td>\n",
       "      <td>7</td>\n",
       "      <td>256</td>\n",
       "      <td>Infinite</td>\n",
       "      <td>38.60</td>\n",
       "      <td>32.84</td>\n",
       "      <td>16.11</td>\n",
       "      <td>44.50</td>\n",
       "      <td>33.09</td>\n",
       "      <td>0.24</td>\n",
       "      <td>11.63</td>\n",
       "      <td>71.08</td>\n",
       "      <td>39.08</td>\n",
       "      <td>28.63</td>\n",
       "      <td>51.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>130</td>\n",
       "      <td>131</td>\n",
       "      <td>[rubert-tiny2](https://huggingface.co/cointegr...</td>\n",
       "      <td>99</td>\n",
       "      <td>29</td>\n",
       "      <td>312</td>\n",
       "      <td>2048</td>\n",
       "      <td>34.88</td>\n",
       "      <td>30.29</td>\n",
       "      <td>22.65</td>\n",
       "      <td>41.17</td>\n",
       "      <td>29.97</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>15.27</td>\n",
       "      <td>69.60</td>\n",
       "      <td>34.58</td>\n",
       "      <td>14.35</td>\n",
       "      <td>45.13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  Rank (Borda)  \\\n",
       "146         146           147   \n",
       "127         127           128   \n",
       "138         138           139   \n",
       "124         124           125   \n",
       "130         130           131   \n",
       "\n",
       "                                                 Model  Zero-shot  \\\n",
       "146  [ternary-weight-embedding](https://huggingface...         -1   \n",
       "127  [potion-base-4M](https://huggingface.co/minish...         99   \n",
       "138  [potion-base-2M](https://huggingface.co/minish...         99   \n",
       "124  [potion-base-8M](https://huggingface.co/minish...         99   \n",
       "130  [rubert-tiny2](https://huggingface.co/cointegr...         99   \n",
       "\n",
       "     Number of Parameters Embedding Dimensions Max Tokens  Mean (Task)  \\\n",
       "146                    98                 1024        512        31.19   \n",
       "127                     3                  128   Infinite        37.86   \n",
       "138                     2                   64   Infinite        36.33   \n",
       "124                     7                  256   Infinite        38.60   \n",
       "130                    29                  312       2048        34.88   \n",
       "\n",
       "     Mean (TaskType)  Bitext Mining  Classification  Clustering  \\\n",
       "146            26.71          12.65           39.03       24.85   \n",
       "127            32.29          14.59           43.71       32.90   \n",
       "138            31.06          12.19           42.25       31.79   \n",
       "124            32.84          16.11           44.50       33.09   \n",
       "130            30.29          22.65           41.17       29.97   \n",
       "\n",
       "     Instruction Retrieval  Multilabel Classification  Pair Classification  \\\n",
       "146                   0.92                      12.14                64.88   \n",
       "127                   0.61                      11.69                70.80   \n",
       "138                   0.59                      11.53                70.43   \n",
       "124                   0.24                      11.63                71.08   \n",
       "130                  -0.09                      15.27                69.60   \n",
       "\n",
       "     Reranking  Retrieval    STS  \n",
       "146      32.09      10.63  43.16  \n",
       "127      38.54      27.23  50.57  \n",
       "138      37.38      23.99  49.34  \n",
       "124      39.08      28.63  51.20  \n",
       "130      34.58      14.35  45.13  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_dt[embed_dt['Number of Parameters'].between(1, 100)].sort_values(by=['Instruction Retrieval'], ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_mdls = list(map(lambda x: x.split(']')[0][1:], embed_dt.sort_values(by=['Instruction Retrieval'], ascending=False).head(2)['Model'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gte-Qwen1.5-7B-instruct', 'gte-Qwen2-7B-instruct']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_mdls\n",
    "# considering performance on tasks including Instruction Retrieval, Retrieval and Reranking, pick gte-Qwen2-7B-instruct as embedding model. but its's too large.\n",
    "# minishlab/potion-base-8M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import contextlib\n",
    "import pandas as pd\n",
    "\n",
    "def dump_pickle(file, outdir):\n",
    "    \"\"\"pickle a file to output directory\n",
    "    \"\"\"\n",
    "    f = open(outdir,\"wb\")\n",
    "    pickle.dump(file, f)\n",
    "    \n",
    "def load_pickle(indir):\n",
    "    \"\"\" load a pickle from indir\n",
    "    \"\"\"\n",
    "    f = open(indir,\"rb\")\n",
    "    return pickle.load(f)\n",
    "    \n",
    "def multiple_strreplace(string, replace_dic):\n",
    "    for k,v in replace_dic.items():\n",
    "        string = string.replace(k,v)\n",
    "    return string\n",
    "\n",
    "def parse_queries(qa_fp, replace_dic):\n",
    "    qa = pd.read_csv(qa_fp)\n",
    "    queries = list(map(lambda query: multiple_strreplace(query, replace_dic), qa['question'].values))\n",
    "    return queries\n",
    "    \n",
    "def log(content, logpath):\n",
    "      try:\n",
    "        content = content.replace('<s>[INST] <<SYS>>', 'System role:'),\n",
    "        content = content.replace('<</SYS>>', ''),\n",
    "        content = content.replace('[/INST]', '\\\\n'),\n",
    "      except:\n",
    "          pass\n",
    "      if os.path.exists(logpath):\n",
    "          with open(logpath, 'a') as f:\n",
    "              with contextlib.redirect_stdout(f):\n",
    "                  print(content)\n",
    "      else:\n",
    "          with open(logpath, 'w') as f:\n",
    "              with contextlib.redirect_stdout(f):\n",
    "                 print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install ragatouille"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -qU langchain_community pdfminer.six"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyJOUAes2SKp"
   },
   "source": [
    "## 1. Dense Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What kind of products or services is NVDIA providing?\n",
      "Who are the customers of NVDIA or what types of markets are NVDIA operating in?\n",
      "Who are the competitors of NVDIA?\n",
      "What are the risk factors and uncertainties that could affect the NVDIA's future performance?\n",
      "What is the 2025 revenue of NVDIA?\n",
      "What is the 2024 revenue of NVDIA?\n",
      "What is the 2025 total liabilities?\n",
      "What is the 2025 total shareholders' equity?\n",
      "What is the 2025 total current assets?\n",
      "What is the 2025 total current liabilities?\n",
      "What is the 2025 gross margin?\n"
     ]
    }
   ],
   "source": [
    "company_name = 'NVDIA'\n",
    "year1, year2 = 2025, 2024\n",
    "qa_fp = '../inputs/Q-A.csv'\n",
    "replace_dic = {'{company_name}':'NVDIA',\n",
    "              '{year1}':str(year1),\n",
    "               '{year2}':str(year2)}\n",
    "querys = parse_queries(qa_fp, replace_dic)\n",
    "dump_pickle(querys, './querys.pck')\n",
    "\n",
    "for q in querys:\n",
    "    print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wqxrqhml0Jzb",
    "outputId": "835edad2-f69d-47a7-f4ec-01bad789f24c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "#from huggingface_hub import login\n",
    "from pathlib import Path\n",
    "dotenv_path = Path('../keys/.env')\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = hf_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store', 'nvda-20250126.pdf', '.ipynb_checkpoints']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'nvda-20250126.pdf'.endswith('pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jUhQa3K5xWLB"
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader, PyPDFLoader, PDFMinerLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "from langchain.llms import HuggingFaceEndpoint\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.retrievers import ParentDocumentRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "querys = load_pickle('./querys.pck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uVmkOx1DaFab",
    "outputId": "ed9ff943-bbbd-4497-a786-ff3b7b19dbbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118 5167\n"
     ]
    }
   ],
   "source": [
    "file_path = \"../data/nvda-20250126.pdf\"\n",
    "#data = UnstructuredPDFLoader(file_path) version conflicts\n",
    "#data = PyPDFLoader(file_path)\n",
    "data = PDFMinerLoader(file_path, mode='page')\n",
    "content = data.load()\n",
    "print(len(content), len(content[0].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Pp5pY9gd1ZK5"
   },
   "outputs": [],
   "source": [
    "parent_chunk_size = 800\n",
    "child_chunk_size = 100\n",
    "\n",
    "#embedding_model_name = 'minishlab/potion-base-8M'\n",
    "embedding_model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "# https://huggingface.co/ng3owb/finance_embedding_8k to test\n",
    "#embedding_model_name = \"mixedbread-ai/mxbai-embed-large-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "VNFQ2-JJrytr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Nan/miniconda3/envs/rag/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model_name,\n",
    ")\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=parent_chunk_size,chunk_overlap=0)\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=child_chunk_size,chunk_overlap=0)\n",
    "#texts = [\"Hello, world!\", \"How are you?\"]\n",
    "#embed_test = embeddings.embed_documents(texts)\n",
    "#import numpy as np\n",
    "#np.shape(embed_test)\n",
    "vectorstore = Chroma(embedding_function=embeddings)\n",
    "store = InMemoryStore()\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "DzE7_fXdrywQ"
   },
   "outputs": [],
   "source": [
    "retriever.add_documents(content,ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yQ0ikrIAeZ14",
    "outputId": "cd0c70cd-fb83-42a5-ca16-0dfa00750bb6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s6/brjlg7nx3m5140p_l6xfby1r0000gn/T/ipykernel_4549/3366330063.py:5: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  relevant_context = retriever.get_relevant_documents(query)\n"
     ]
    }
   ],
   "source": [
    "logfp = '../outputs/pdfminer_retrieval_log.txt'\n",
    "log('', logfp)\n",
    "log(f'parent_chunk_size:{parent_chunk_size}, child_chunk_size:{child_chunk_size}, embed_model:{embedding_model_name}', logfp)\n",
    "for query in querys:\n",
    "    relevant_context = retriever.get_relevant_documents(query)\n",
    "    log(f\"retrieve_instruction:{query}\", logfp)\n",
    "    for d in relevant_context:\n",
    "        log(f\"retrieved_content:{d.page_content}\", logfp)\n",
    "        log('', logfp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ColBERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_document_length = 512\n",
    "documents = list(map(lambda x: x.page_content, content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Nan/miniconda3/envs/rag/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 08, 02:11:12] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Nan/miniconda3/envs/rag/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "# https://github.com/AnswerDotAI/RAGatouille\n",
    "RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- WARNING! You are using PLAID with an experimental replacement for FAISS for greater compatibility ----\n",
      "This is a behaviour change from RAGatouille 0.8.0 onwards.\n",
      "This works fine for most users and smallish datasets, but can be considerably slower than FAISS and could cause worse results in some situations.\n",
      "If you're confident with FAISS working on your machine, pass use_faiss=True to revert to the FAISS-using behaviour.\n",
      "--------------------\n",
      "\n",
      "\n",
      "[Mar 08, 02:11:13] #> Note: Output directory .ragatouille/colbert/indexes/NVDIA already exists\n",
      "\n",
      "\n",
      "[Mar 08, 02:11:13] #> Will delete 10 files already at .ragatouille/colbert/indexes/NVDIA in 20 seconds...\n",
      "[Mar 08, 02:11:34] [0] \t\t #> Encoding 274 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]/Users/Nan/miniconda3/envs/rag/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████████| 9/9 [05:52<00:00, 39.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 08, 02:17:27] [0] \t\t avg_doclen_est = 261.6788330078125 \t len(local_sample) = 274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 08, 02:17:27] [0] \t\t Creating 4,096 partitions.\n",
      "[Mar 08, 02:17:27] [0] \t\t *Estimated* 71,700 embeddings.\n",
      "[Mar 08, 02:17:27] [0] \t\t #> Saving the indexing plan to .ragatouille/colbert/indexes/NVDIA/plan.json ..\n",
      "used 20 iterations (73.6211s) to cluster 68115 items into 4096 clusters\n",
      "[0.026, 0.03, 0.029, 0.027, 0.029, 0.028, 0.027, 0.028, 0.027, 0.028, 0.026, 0.027, 0.028, 0.03, 0.028, 0.028, 0.025, 0.026, 0.028, 0.028, 0.028, 0.028, 0.026, 0.028, 0.026, 0.026, 0.029, 0.027, 0.029, 0.03, 0.027, 0.032, 0.028, 0.026, 0.028, 0.025, 0.028, 0.03, 0.027, 0.032, 0.027, 0.027, 0.027, 0.03, 0.028, 0.026, 0.026, 0.032, 0.032, 0.027, 0.025, 0.028, 0.029, 0.027, 0.026, 0.028, 0.034, 0.029, 0.034, 0.027, 0.026, 0.028, 0.028, 0.03, 0.03, 0.029, 0.031, 0.029, 0.025, 0.028, 0.029, 0.025, 0.027, 0.03, 0.027, 0.029, 0.029, 0.029, 0.027, 0.031, 0.031, 0.029, 0.028, 0.028, 0.028, 0.026, 0.027, 0.029, 0.028, 0.032, 0.029, 0.031, 0.027, 0.029, 0.028, 0.028, 0.031, 0.027, 0.028, 0.028, 0.028, 0.032, 0.028, 0.027, 0.028, 0.028, 0.027, 0.026, 0.027, 0.027, 0.029, 0.028, 0.029, 0.028, 0.03, 0.028, 0.028, 0.031, 0.027, 0.028, 0.027, 0.028, 0.027, 0.03, 0.026, 0.031, 0.028, 0.027]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 08, 02:18:41] [0] \t\t #> Encoding 274 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      " 11%|█████                                        | 1/9 [00:37<04:59, 37.42s/it]\u001b[A\n",
      " 22%|██████████                                   | 2/9 [01:22<04:54, 42.03s/it]\u001b[A\n",
      " 33%|███████████████                              | 3/9 [02:04<04:11, 41.96s/it]\u001b[A\n",
      " 44%|████████████████████                         | 4/9 [02:51<03:40, 44.12s/it]\u001b[A\n",
      " 56%|█████████████████████████                    | 5/9 [03:35<02:55, 43.98s/it]\u001b[A\n",
      " 67%|██████████████████████████████               | 6/9 [04:16<02:08, 42.89s/it]\u001b[A\n",
      " 78%|███████████████████████████████████          | 7/9 [04:54<01:22, 41.32s/it]\u001b[A\n",
      " 89%|████████████████████████████████████████     | 8/9 [05:29<00:39, 39.42s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 9/9 [05:52<00:00, 39.18s/it]\u001b[A\n",
      "1it [05:57, 357.07s/it]\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 402.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 08, 02:24:38] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Mar 08, 02:24:38] #> Building the emb2pid mapping..\n",
      "[Mar 08, 02:24:38] len(emb2pid) = 71700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████| 4096/4096 [00:01<00:00, 3339.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 08, 02:24:40] #> Saved optimized IVF to .ragatouille/colbert/indexes/NVDIA/ivf.pid.pt\n",
      "Done indexing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "RAG.index(\n",
    "    collection=documents,\n",
    "    index_name=\"NVDIA\",\n",
    "    max_document_length=max_document_length,\n",
    "    split_documents=True,\n",
    ")\n",
    "clbt_retriever = RAG.as_langchain_retriever(k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yQ0ikrIAeZ14",
    "outputId": "cd0c70cd-fb83-42a5-ca16-0dfa00750bb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading searcher for index NVDIA for the first time... This may take a few seconds\n",
      "[Mar 08, 00:51:25] #> Loading codec...\n",
      "[Mar 08, 00:51:25] #> Loading IVF...\n",
      "[Mar 08, 00:51:25] Loading segmented_lookup_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Nan/miniconda3/envs/rag/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 08, 00:51:25] #> Loading doclens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 1187.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 08, 00:51:25] #> Loading codes and residuals...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 113.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 08, 00:51:25] Loading filter_pids_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 08, 00:51:25] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "Searcher loaded!\n",
      "\n",
      "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "#> Input: What kind of products or services is NVDIA providing?, \t\t True, \t\t None\n",
      "#> Output IDs: torch.Size([32]), tensor([  101,     1,  2054,  2785,  1997,  3688,  2030,  2578,  2003,  1050,\n",
      "        16872,  2401,  4346,  1029,   102,   103,   103,   103,   103,   103,\n",
      "          103,   103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
      "          103,   103])\n",
      "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Nan/miniconda3/envs/rag/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/Nan/miniconda3/envs/rag/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/Nan/miniconda3/envs/rag/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/Nan/miniconda3/envs/rag/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/Nan/miniconda3/envs/rag/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/Nan/miniconda3/envs/rag/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/Nan/miniconda3/envs/rag/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/Nan/miniconda3/envs/rag/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/Nan/miniconda3/envs/rag/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/Nan/miniconda3/envs/rag/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/Nan/miniconda3/envs/rag/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "logfp = '../outputs/pdfminer_retrieval_log.txt'\n",
    "log('', logfp)\n",
    "log(f'max_document_length:{max_document_length}, embed_model:colbertv2.0', logfp)\n",
    "for query in querys:\n",
    "    relevant_context = clbt_retriever.get_relevant_documents(query)\n",
    "    log(f\"retrieve_instruction:{query}\", logfp)\n",
    "    for d in relevant_context:\n",
    "        log(f\"retrieved_content:{d.page_content}\", logfp)\n",
    "        log('', logfp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hybrid retriever + rerank\n",
    "https://haystack.deepset.ai/blog/hybrid-retrieval\n",
    "Popular rerank models include Cohere rerank, bge-reranker, among others.\n",
    "bm25_retriever\n",
    "\n",
    "https://superlinked.com/vectorhub/articles/optimizing-rag-with-hybrid-search-reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain langchain-community rank_bm25 pypdf unstructured chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rank_bm25\n",
      "  Using cached rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy in /Users/Nan/miniconda3/envs/rag/lib/python3.11/site-packages (from rank_bm25) (1.26.4)\n",
      "Using cached rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Installing collected packages: rank_bm25\n",
      "Successfully installed rank_bm25-0.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Pp5pY9gd1ZK5"
   },
   "outputs": [],
   "source": [
    "chunk_size = 200\n",
    "chunk_overlap = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.llms import HuggingFaceHub\n",
    "import torch\n",
    "from transformers import ( AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline, )\n",
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "import os\n",
    "\n",
    "chunk_size = 800\n",
    "chunk_overlap = 30\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,\n",
    "                                          chunk_overlap=chunk_overlap)\n",
    "chunks = splitter.split_documents(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_retriever = BM25Retriever.from_documents(chunks)\n",
    "keyword_retriever.k = 5\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(retrievers=[clbt_retriever,\n",
    "                                                   keyword_retriever],\n",
    "                                       weights=[0.3, 0.7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "rerank_model_name = \"BAAI/bge-reranker-base\"\n",
    "rerank_model = HuggingFaceCrossEncoder(model_name=rerank_model_name)\n",
    "compressor = CrossEncoderReranker(model=rerank_model, top_n=5)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=ensemble_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=vectorstore_retreiver\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yQ0ikrIAeZ14",
    "outputId": "cd0c70cd-fb83-42a5-ca16-0dfa00750bb6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s6/brjlg7nx3m5140p_l6xfby1r0000gn/T/ipykernel_6568/3272588318.py:5: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  relevant_context = compression_retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading searcher for index NVDIA for the first time... This may take a few seconds\n",
      "[Mar 08, 02:28:16] #> Loading codec...\n",
      "[Mar 08, 02:28:16] #> Loading IVF...\n",
      "[Mar 08, 02:28:16] Loading segmented_lookup_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Nan/miniconda3/envs/rag/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 08, 02:28:16] #> Loading doclens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 567.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 08, 02:28:16] #> Loading codes and residuals...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 46.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 08, 02:28:16] Loading filter_pids_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 08, 02:28:17] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "Searcher loaded!\n",
      "\n",
      "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "#> Input: What kind of products or services is NVDIA providing?, \t\t True, \t\t None\n",
      "#> Output IDs: torch.Size([32]), tensor([  101,     1,  2054,  2785,  1997,  3688,  2030,  2578,  2003,  1050,\n",
      "        16872,  2401,  4346,  1029,   102,   103,   103,   103,   103,   103,\n",
      "          103,   103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
      "          103,   103])\n",
      "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Nan/miniconda3/envs/rag/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/Nan/miniconda3/envs/rag/lib/python3.11/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:1699: UserWarning: cumsum_out_mps supported by MPS on MacOS 13+, please upgrade (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/UnaryOps.mm:425.)\n",
      "  incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n",
      "/Users/Nan/miniconda3/envs/rag/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/Nan/miniconda3/envs/rag/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/Nan/miniconda3/envs/rag/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/Nan/miniconda3/envs/rag/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/Nan/miniconda3/envs/rag/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/Nan/miniconda3/envs/rag/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/Nan/miniconda3/envs/rag/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/Nan/miniconda3/envs/rag/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/Nan/miniconda3/envs/rag/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/Nan/miniconda3/envs/rag/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "logfp = '../outputs/pdfminer_retrieval_log.txt'\n",
    "log('', logfp)\n",
    "log(f'chunk_size:{chunk_size}, chunk_overlap:{chunk_overlap}, max_document_length:{max_document_length}, embed_model:colbertv2.0, keyword_model:BM25Retriever, rerank_model:{rerank_model_name}', logfp)\n",
    "for query in querys:\n",
    "    relevant_context = compression_retriever.get_relevant_documents(query)\n",
    "    log(f\"retrieve_instruction:{query}\", logfp)\n",
    "    for d in relevant_context:\n",
    "        log(f\"retrieved_content:{d.page_content}\", logfp)\n",
    "        log('', logfp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Pp5pY9gd1ZK5"
   },
   "outputs": [],
   "source": [
    "parent_chunk_size = 800\n",
    "child_chunk_size = 100\n",
    "embedding_model_name = \"BAAI/bge-base-en-v1.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=parent_chunk_size,chunk_overlap=0)\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=child_chunk_size,chunk_overlap=0)\n",
    "#texts = [\"Hello, world!\", \"How are you?\"]\n",
    "#embed_test = embeddings.embed_documents(texts)\n",
    "#import numpy as np\n",
    "#np.shape(embed_test)\n",
    "vectorstore = Chroma(embedding_function=embeddings)\n",
    "store = InMemoryStore()\n",
    "vectorstore_retreiver = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    "    k=5\n",
    ")\n",
    "vectorstore_retreiver.add_documents(content,ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=parent_chunk_size,chunk_overlap=20)\n",
    "chunks = splitter.split_documents(content)\n",
    "keyword_retriever = BM25Retriever.from_documents(chunks)\n",
    "keyword_retriever.k = 5\n",
    "ensemble_retriever = EnsembleRetriever(retrievers=[vectorstore_retreiver,\n",
    "                                                   keyword_retriever],\n",
    "                                       weights=[0.3, 0.7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "rerank_model_name = \"BAAI/bge-reranker-base\"\n",
    "rerank_model = HuggingFaceCrossEncoder(model_name=rerank_model_name)\n",
    "compressor = CrossEncoderReranker(model=rerank_model, top_n=5)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=vectorstore_retreiver\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=vectorstore_retreiver\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yQ0ikrIAeZ14",
    "outputId": "cd0c70cd-fb83-42a5-ca16-0dfa00750bb6"
   },
   "outputs": [],
   "source": [
    "logfp = '../outputs/retrieval_log.txt'\n",
    "log('', logfp)\n",
    "log(f'parent_chunk_size:{parent_chunk_size}, child_chunk_size:{child_chunk_size}, embed_model:{embedding_model_name}, keyword_model:BM25Retriever, rerank_model:{rerank_model_name}', logfp)\n",
    "for query in querys:\n",
    "    relevant_context = compression_retriever.get_relevant_documents(query)\n",
    "    log(f\"retrieve_instruction:{query}\", logfp)\n",
    "    for d in relevant_context:\n",
    "        log(f\"retrieved_content:{d.page_content}\", logfp)\n",
    "        log('', logfp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBJK03bFlPYB"
   },
   "source": [
    "## Step-1 Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2tdp6NitetcP"
   },
   "outputs": [],
   "source": [
    "os.environ[\"COHERE_API_KEY\"] = \"5uuX8mk9dhf9KHzw7vSDhQdXlV2x92MzELvJ972T\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZKeCsvuEhd8n"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R840R1tYiwjR"
   },
   "outputs": [],
   "source": [
    "from cohere import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IiGXuxKli5D5"
   },
   "outputs": [],
   "source": [
    "co = Client(api_key = \"5uuX8mk9dhf9KHzw7vSDhQdXlV2x92MzELvJ972T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CM4n_Gc1jD6z"
   },
   "outputs": [],
   "source": [
    "from typing import ForwardRef\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class CustomCohereRerank(CohereRerank):\n",
    "  class Config(BaseModel.Config):\n",
    "    arbitrary_types_allowed = True\n",
    "\n",
    "CustomCohereRerank.update_forward_refs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jcjuTSAhoxr"
   },
   "outputs": [],
   "source": [
    "compressor = CustomCohereRerank(client=co)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "btAqdGZTheAU"
   },
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_q4YJ_G8lJ4r"
   },
   "source": [
    "## Step - 2 Augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "CgYVFFL1kqou"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "1U9KA37W4yjf"
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "<|system|>>\n",
    "You are an AI Assistant that follows instructions extremely well.\n",
    "Please be truthful and give direct answers. Please tell 'I don't know' if user query is not in CONTEXT\n",
    "\n",
    "CONTEXT: {context}\n",
    "</s>\n",
    "<|user|>\n",
    "{query}\n",
    "</s>\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "1U9KA37W4yjf"
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "<|system|>>\n",
    "You are an AI Assistant that follows instructions extremely well.\n",
    "Please be truthful and give direct answers. Please tell 'I don't know' if user query is not in CONTEXT\n",
    "\n",
    "</s>\n",
    "<|user|>\n",
    "{query}\n",
    "</s>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "query = \"What is the revenue of nvdia in 2023?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "WDkCjEcTkqLO"
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = prompt | llm\n",
    "print(llm_chain.invoke({\"query\": query}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mk0Km6jO4sJw"
   },
   "source": [
    "## Step-3 Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0fhcy_KlkYEe"
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rDpth0wTkms5"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IxnclNJ8k3tL"
   },
   "outputs": [],
   "source": [
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mUGKQyG_kn0O"
   },
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"query\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wNFH95K2lUY3"
   },
   "outputs": [],
   "source": [
    "query = \"Who is Rahul?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XcJEurHfk5xM"
   },
   "outputs": [],
   "source": [
    "response = chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m6X58EFrk703",
    "outputId": "7d840433-c8b4-4941-9a1c-29a58bf15c16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do not have information about a specific person named rahul. please provide more context or information about rahul to help me identify who you are referring to.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-VTZErjXlbH4",
    "outputId": "caf1ad1e-4a5f-4bec-a0f8-f2907a423f0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tarun's role at AI Planet is \"Developer Relations and Community Manager.\" (from the provided context)\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"what is Tarun's role at AI Planet?\"))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
